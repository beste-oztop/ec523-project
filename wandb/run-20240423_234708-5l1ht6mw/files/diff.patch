diff --git a/algorithms/__pycache__/PPO.cpython-39.pyc b/algorithms/__pycache__/PPO.cpython-39.pyc
index 05cf574..3ef9739 100644
Binary files a/algorithms/__pycache__/PPO.cpython-39.pyc and b/algorithms/__pycache__/PPO.cpython-39.pyc differ
diff --git a/main.py b/main.py
index e293bbd..c81b853 100644
--- a/main.py
+++ b/main.py
@@ -33,7 +33,7 @@ class Args:
     cuda: bool = True
     """if toggled, cuda will be enabled by default"""
 
-    track: bool = False
+    track: bool = True
     """if toggled, this experiment will be tracked with Weights and Biases"""
     wandb_project_name: str = f"ec523"
     """the wandb's project name"""
@@ -46,8 +46,8 @@ class Args:
     upload_model: bool = False
     """whether to upload the saved model to huggingface"""
 
-    # Algorithm specific arguments
-    env_id: str = "Humanoid-v4"
+    # DQN specific arguments
+    env_id: str = "Pendulum-v1"
     """the id of the environment"""
     total_timesteps: int = 500000
     """total timesteps of the experiments"""
@@ -76,6 +76,49 @@ class Args:
     train_frequency: int = 10
     """the frequency of training"""
 
+    # PPO specific arguments
+
+    total_timesteps: int = 1000000
+    """total timesteps of the experiments"""
+    learning_rate: float = 3e-4
+    """the learning rate of the optimizer"""
+    num_envs: int = 1
+    """the number of parallel game environments"""
+    num_steps: int = 2048
+    """the number of steps to run in each environment per policy rollout"""
+    anneal_lr: bool = True
+    """Toggle learning rate annealing for policy and value networks"""
+    gamma: float = 0.99
+    """the discount factor gamma"""
+    gae_lambda: float = 0.95
+    """the lambda for the general advantage estimation"""
+    num_minibatches: int = 32
+    """the number of mini-batches"""
+    update_epochs: int = 10
+    """the K epochs to update the policy"""
+    norm_adv: bool = True
+    """Toggles advantages normalization"""
+    clip_coef: float = 0.2
+    """the surrogate clipping coefficient"""
+    clip_vloss: bool = True
+    """Toggles whether or not to use a clipped loss for the value function, as per the paper."""
+    ent_coef: float = 0.0
+    """coefficient of the entropy"""
+    vf_coef: float = 0.5
+    """coefficient of the value function"""
+    max_grad_norm: float = 0.5
+    """the maximum norm for the gradient clipping"""
+    target_kl: float = None
+    """the target KL divergence threshold"""
+
+    # to be filled in runtime
+    batch_size: int = 0
+    """the batch size (computed in runtime)"""
+    minibatch_size: int = 0
+    """the mini-batch size (computed in runtime)"""
+    num_iterations: int = 0
+    """the number of iterations (computed in runtime)"""
+
 
 def make_env(env_id, seed, idx, capture_video, run_name, args):
     def thunk():
diff --git a/wandb/debug-internal.log b/wandb/debug-internal.log
index d4ed6f6..ee89d18 120000
--- a/wandb/debug-internal.log
+++ b/wandb/debug-internal.log
@@ -1 +1 @@
-run-20240423_202847-9gtp2rpe/logs/debug-internal.log
\ No newline at end of file
+run-20240423_234708-5l1ht6mw/logs/debug-internal.log
\ No newline at end of file
diff --git a/wandb/debug.log b/wandb/debug.log
index feeb4c1..a1ca093 120000
--- a/wandb/debug.log
+++ b/wandb/debug.log
@@ -1 +1 @@
-run-20240423_202847-9gtp2rpe/logs/debug.log
\ No newline at end of file
+run-20240423_234708-5l1ht6mw/logs/debug.log
\ No newline at end of file
diff --git a/wandb/latest-run b/wandb/latest-run
index 9dc540a..b97eea0 120000
--- a/wandb/latest-run
+++ b/wandb/latest-run
@@ -1 +1 @@
-run-20240423_202847-9gtp2rpe
\ No newline at end of file
+run-20240423_234708-5l1ht6mw
\ No newline at end of file
