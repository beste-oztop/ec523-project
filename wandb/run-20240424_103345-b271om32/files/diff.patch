diff --git a/algorithms/PPO.py b/algorithms/PPO.py
index 09cd6ab..a4260b7 100644
--- a/algorithms/PPO.py
+++ b/algorithms/PPO.py
@@ -13,7 +13,7 @@ from torch.distributions.normal import Normal
 from torch.utils.tensorboard import SummaryWriter
 
 from buffers import ReplayBuffer
-from networks import AgentPPO
+from networks import PPONetwork
 
 class PPO():
 
@@ -30,7 +30,7 @@ class PPO():
 
         self.envs = envs
         self.writer = writer
-        self.agent = AgentPPO(envs).to(self.device)
+        self.agent = PPONetwork(envs).to(self.device)
         self.optimizer = optim.Adam(self.agent.parameters(), lr=args.learning_rate)
 
 
diff --git a/algorithms/__pycache__/PPO.cpython-39.pyc b/algorithms/__pycache__/PPO.cpython-39.pyc
index 05cf574..4f2ea05 100644
Binary files a/algorithms/__pycache__/PPO.cpython-39.pyc and b/algorithms/__pycache__/PPO.cpython-39.pyc differ
diff --git a/main.py b/main.py
index e293bbd..c8550fb 100644
--- a/main.py
+++ b/main.py
@@ -23,7 +23,7 @@ class Args:
     """ name of the algorithm"""
     available_network_modes = ["default", "deeper", "wider", "mini"]
     """list of all available network mode"""
-    network_mode = "mini"
+    network_mode = "default"
     """the active network mode"""
 
     seed: int = 1
@@ -33,7 +33,7 @@ class Args:
     cuda: bool = True
     """if toggled, cuda will be enabled by default"""
 
-    track: bool = False
+    track: bool = True
     """if toggled, this experiment will be tracked with Weights and Biases"""
     wandb_project_name: str = f"ec523"
     """the wandb's project name"""
@@ -46,8 +46,8 @@ class Args:
     upload_model: bool = False
     """whether to upload the saved model to huggingface"""
 
-    # Algorithm specific arguments
-    env_id: str = "Humanoid-v4"
+    # DQN specific arguments
+    env_id: str = "Pendulum-v1"
     """the id of the environment"""
     total_timesteps: int = 500000
     """total timesteps of the experiments"""
@@ -76,6 +76,49 @@ class Args:
     train_frequency: int = 10
     """the frequency of training"""
 
+    # PPO specific arguments
+
+    total_timesteps: int = 1000000
+    """total timesteps of the experiments"""
+    learning_rate: float = 3e-4
+    """the learning rate of the optimizer"""
+    num_envs: int = 1
+    """the number of parallel game environments"""
+    num_steps: int = 2048
+    """the number of steps to run in each environment per policy rollout"""
+    anneal_lr: bool = True
+    """Toggle learning rate annealing for policy and value networks"""
+    gamma: float = 0.99
+    """the discount factor gamma"""
+    gae_lambda: float = 0.95
+    """the lambda for the general advantage estimation"""
+    num_minibatches: int = 32
+    """the number of mini-batches"""
+    update_epochs: int = 10
+    """the K epochs to update the policy"""
+    norm_adv: bool = True
+    """Toggles advantages normalization"""
+    clip_coef: float = 0.2
+    """the surrogate clipping coefficient"""
+    clip_vloss: bool = True
+    """Toggles whether or not to use a clipped loss for the value function, as per the paper."""
+    ent_coef: float = 0.0
+    """coefficient of the entropy"""
+    vf_coef: float = 0.5
+    """coefficient of the value function"""
+    max_grad_norm: float = 0.5
+    """the maximum norm for the gradient clipping"""
+    target_kl: float = None
+    """the target KL divergence threshold"""
+
+    # to be filled in runtime
+    batch_size: int = 0
+    """the batch size (computed in runtime)"""
+    minibatch_size: int = 0
+    """the mini-batch size (computed in runtime)"""
+    num_iterations: int = 0
+    """the number of iterations (computed in runtime)"""
+
 
 def make_env(env_id, seed, idx, capture_video, run_name, args):
     def thunk():
@@ -144,8 +187,10 @@ if __name__ == '__main__':
         dqn_agent.execute(args)
 
     elif args.alg_name == "PPO":
-        dqn_agent = PPO(envs, writer, args)
-        dqn_agent.execute(args)
+        print("inside PPO")
+        ppo_agent = PPO(envs, writer, args)
+        print("before PPO")
+        ppo_agent.execute(args)
 
     else:
         raise NotImplementedError
diff --git a/networks/__init__.py b/networks/__init__.py
index fd05bc0..6605898 100644
--- a/networks/__init__.py
+++ b/networks/__init__.py
@@ -87,7 +87,7 @@ class WiderQNetwork(nn.Module):
 
 
 # Agent network implementation for PPO
-class AgentPPO(nn.Module):
+class PPONetwork(nn.Module):
     def __init__(self, envs):
         super().__init__()
         self.critic = nn.Sequential(
diff --git a/networks/__pycache__/__init__.cpython-39.pyc b/networks/__pycache__/__init__.cpython-39.pyc
index 9c242a3..53915ac 100644
Binary files a/networks/__pycache__/__init__.cpython-39.pyc and b/networks/__pycache__/__init__.cpython-39.pyc differ
diff --git a/wandb/debug-cli.aida.log b/wandb/debug-cli.aida.log
index 4468a63..87230f4 100644
--- a/wandb/debug-cli.aida.log
+++ b/wandb/debug-cli.aida.log
@@ -3458,3 +3458,22 @@
 2024-04-23 20:28:52 INFO No path found after runs/PPO_CartPole-v1_mini_network_1_1713918525/events.out.tfevents.1713918528.Aidas-MacBook.85177.0
 2024-04-23 20:28:53 INFO No path found after runs/PPO_CartPole-v1_mini_network_1_1713918525/events.out.tfevents.1713918528.Aidas-MacBook.85177.0
 2024-04-23 20:28:54 INFO No path found after runs/PPO_CartPole-v1_mini_network_1_1713918525/events.out.tfevents.1713918528.Aidas-MacBook.85177.0
+2024-04-23 23:47:09 INFO No path found after runs/PPO_Pendulum-v1_mini_network_1_1713930425/events.out.tfevents.1713930429.Aidas-MacBook.9438.0
+2024-04-23 23:47:10 INFO No path found after runs/PPO_Pendulum-v1_mini_network_1_1713930425/events.out.tfevents.1713930429.Aidas-MacBook.9438.0
+2024-04-23 23:47:10 INFO No path found after runs/PPO_Pendulum-v1_mini_network_1_1713930425/events.out.tfevents.1713930429.Aidas-MacBook.9438.0
+2024-04-23 23:47:10 INFO No path found after runs/PPO_Pendulum-v1_mini_network_1_1713930425/events.out.tfevents.1713930429.Aidas-MacBook.9438.0
+2024-04-23 23:47:11 INFO No path found after runs/PPO_Pendulum-v1_mini_network_1_1713930425/events.out.tfevents.1713930429.Aidas-MacBook.9438.0
+2024-04-23 23:47:12 INFO No path found after runs/PPO_Pendulum-v1_mini_network_1_1713930425/events.out.tfevents.1713930429.Aidas-MacBook.9438.0
+2024-04-23 23:47:13 INFO No path found after runs/PPO_Pendulum-v1_mini_network_1_1713930425/events.out.tfevents.1713930429.Aidas-MacBook.9438.0
+2024-04-23 23:47:14 INFO No path found after runs/PPO_Pendulum-v1_mini_network_1_1713930425/events.out.tfevents.1713930429.Aidas-MacBook.9438.0
+2024-04-23 23:47:15 INFO No path found after runs/PPO_Pendulum-v1_mini_network_1_1713930425/events.out.tfevents.1713930429.Aidas-MacBook.9438.0
+2024-04-23 23:47:16 INFO No path found after runs/PPO_Pendulum-v1_mini_network_1_1713930425/events.out.tfevents.1713930429.Aidas-MacBook.9438.0
+2024-04-24 10:28:59 INFO No path found after runs/PPO_Pendulum-v1_default_network_1_1713968937/events.out.tfevents.1713968939.Aidas-MacBook.11923.0
+2024-04-24 10:29:00 INFO No path found after runs/PPO_Pendulum-v1_default_network_1_1713968937/events.out.tfevents.1713968939.Aidas-MacBook.11923.0
+2024-04-24 10:29:00 INFO No path found after runs/PPO_Pendulum-v1_default_network_1_1713968937/events.out.tfevents.1713968939.Aidas-MacBook.11923.0
+2024-04-24 10:29:00 INFO No path found after runs/PPO_Pendulum-v1_default_network_1_1713968937/events.out.tfevents.1713968939.Aidas-MacBook.11923.0
+2024-04-24 10:29:01 INFO No path found after runs/PPO_Pendulum-v1_default_network_1_1713968937/events.out.tfevents.1713968939.Aidas-MacBook.11923.0
+2024-04-24 10:29:02 INFO No path found after runs/PPO_Pendulum-v1_default_network_1_1713968937/events.out.tfevents.1713968939.Aidas-MacBook.11923.0
+2024-04-24 10:29:03 INFO No path found after runs/PPO_Pendulum-v1_default_network_1_1713968937/events.out.tfevents.1713968939.Aidas-MacBook.11923.0
+2024-04-24 10:29:04 INFO No path found after runs/PPO_Pendulum-v1_default_network_1_1713968937/events.out.tfevents.1713968939.Aidas-MacBook.11923.0
+2024-04-24 10:29:05 INFO No path found after runs/PPO_Pendulum-v1_default_network_1_1713968937/events.out.tfevents.1713968939.Aidas-MacBook.11923.0
diff --git a/wandb/debug-internal.log b/wandb/debug-internal.log
index d4ed6f6..422cdc7 120000
--- a/wandb/debug-internal.log
+++ b/wandb/debug-internal.log
@@ -1 +1 @@
-run-20240423_202847-9gtp2rpe/logs/debug-internal.log
\ No newline at end of file
+run-20240424_103345-b271om32/logs/debug-internal.log
\ No newline at end of file
diff --git a/wandb/debug.log b/wandb/debug.log
index feeb4c1..ef3bc7e 120000
--- a/wandb/debug.log
+++ b/wandb/debug.log
@@ -1 +1 @@
-run-20240423_202847-9gtp2rpe/logs/debug.log
\ No newline at end of file
+run-20240424_103345-b271om32/logs/debug.log
\ No newline at end of file
diff --git a/wandb/latest-run b/wandb/latest-run
index 9dc540a..60ff855 120000
--- a/wandb/latest-run
+++ b/wandb/latest-run
@@ -1 +1 @@
-run-20240423_202847-9gtp2rpe
\ No newline at end of file
+run-20240424_103345-b271om32
\ No newline at end of file
